{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"./new_train/new_train/\"\n",
    "test_path = \"./new_val_in/new_val_in/\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "val_dataset  = ArgoverseDataset(data_path=new_path)\n",
    "test_dataset = ArgoverseDataset(data_path=test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 512\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    #inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = [numpy.dstack([scene['p_in']]) for scene in batch]\n",
    "    #out = [numpy.dstack([scene['p_out'], scene['v_out']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out']]) for scene in batch]\n",
    "    agent_id = [[scene['agent_id']] for scene in batch]\n",
    "    track_id = [numpy.dstack([scene['track_id']]) for scene in batch]\n",
    "    inp = torch.Tensor(inp).float()\n",
    "    out = torch.Tensor(out).float()\n",
    "    return [inp, out, track_id, agent_id]\n",
    "    #return [inp, out]\n",
    "\n",
    "def test_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    #inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    inp = [numpy.dstack([scene['p_in']]) for scene in batch]\n",
    "    agent_id = [[scene['agent_id']] for scene in batch]\n",
    "    track_id = [numpy.dstack([scene['track_id']]) for scene in batch]\n",
    "    inp = torch.Tensor(inp).float()\n",
    "    return [inp, track_id, agent_id]\n",
    "    #return [inp, out]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = True, collate_fn=my_collate, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset,batch_size=3200, shuffle = False, collate_fn=test_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class myModel(nn.Module):\n",
    "    \"\"\"This class defines your deep learning model that extends a Module class\n",
    "      The constructor of your class defines the layers of the model. \n",
    "      The forward() function defines how to forward propagate \n",
    "      input through the defined layers of the model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(myModel, self).__init__()\n",
    "        #self.linear1 = nn.Linear(19, 240*30)\n",
    "        #self.relu = nn.ReLU()\n",
    "        #self.linear2 = nn.Linear(240*30, 30)\n",
    "        \n",
    "        self.linear1 = nn.Linear(19, 240*30)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(240*30, 30)\n",
    "        \n",
    "        #self.hidden_dim = 2048*2\n",
    "        #self.num_layers = 3\n",
    "        #self.lstm = nn.LSTM(240, self.hidden_dim, num_layers = self.num_layers, batch_first = True)\n",
    "        #self.linear = nn.Conv1d(self.hidden_dim, 240, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        #x, _ = self.lstm(x)\n",
    "        #x = x.transpose(1, 2)\n",
    "        #x = self.linear(x)\n",
    "        #x = x.transpose(1, 2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.linear1(x)\n",
    "            x = self.relu(x)\n",
    "            x = self.linear2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myModel()\n",
    "learning_rate = 1e-3\n",
    "l = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr =learning_rate )\n",
    "\n",
    "loss_ema = -1\n",
    "model.train()\n",
    "for epoch in range(1500):\n",
    "    for i_batch, sample_batch in enumerate(val_loader):\n",
    "        inp, out = sample_batch\n",
    "        \n",
    "        inp = inp.float()\n",
    "        out = out.float()\n",
    "        \n",
    "        mixed = torch.cat([inp, out],2).transpose(1, 2).reshape(-1, 49, 240).float()\n",
    "        y_pred = model(mixed[:, :-1,])[:, -30:]\n",
    "        y_pred = y_pred.reshape((-1, 30,60, 4)).transpose(1, 2)\n",
    "        \n",
    "        loss = l(y_pred, out) ** 0.5\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss_ema < 0:\n",
    "            loss_ema = loss\n",
    "        loss_ema = loss_ema*0.99+loss*0.01\n",
    "        \n",
    "        if epoch%25 == 0:\n",
    "            print('epoch {}, loss_ema {}, loss {}'.format(epoch, loss_ema.item(), loss.item()))\n",
    "            \n",
    "        break\n",
    "        \n",
    "print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for epoch in range(2):\n",
    "    for i_batch, sample_batch in enumerate(test_loader):\n",
    "        inp, out, track_id, agents = sample_batch\n",
    "        flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "        inp = inp.type(torch.cuda.FloatTensor)\n",
    "        \n",
    "        mixed = torch.cat([inp, out],2).transpose(1, 2).reshape(-1, 49, 240)\n",
    "        y_pred = model(inp.reshape(-1, 49, 240))[:, -30:]\n",
    "        y_pred = y_pred.reshape((-1, 30,60, 4)).transpose(1, 2)\n",
    "        \n",
    "        print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss_ema 2211.722412109375, loss 2211.722412109375\n",
      "epoch 25, loss_ema 1819.86376953125, loss 105.49495697021484\n",
      "epoch 50, loss_ema 1430.7257080078125, loss 37.456146240234375\n",
      "epoch 75, loss_ema 1118.08154296875, loss 18.681961059570312\n",
      "epoch 100, loss_ema 873.8159790039062, loss 18.346202850341797\n",
      "epoch 125, loss_ema 683.6777954101562, loss 17.843725204467773\n",
      "epoch 150, loss_ema 535.702392578125, loss 19.07477569580078\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-7974453f8c6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mflat_track_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-227c739d357a>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mpkl_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpkl_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkl_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = myModel()\n",
    "model.train()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "l = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr =learning_rate )\n",
    "loss_ema = -1\n",
    "\n",
    "for epoch in range(2000):\n",
    "    \n",
    "    correct = 0\n",
    "    for i_batch, sample_batch in enumerate(val_loader):\n",
    "        inp, out, track_id, agents = sample_batch\n",
    "        flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "        inp = inp #.cuda()\n",
    "        out = out #.cuda()\n",
    "        \n",
    "        batch_sz = inp.size(0)\n",
    "        agent_sz = inp.size(1)\n",
    "        \n",
    "        predict_in = []\n",
    "        y_train = []\n",
    "        \n",
    "        \n",
    "        for i in range(batch_sz):\n",
    "            \n",
    "            agent_id = numpy.where(flat_track_id[i][:] == (agents[i]))[0][0]\n",
    "            #if i ==0:\n",
    "                #print(agents[i])\n",
    "            predict_in.append(inp[i, agent_id,:, 0:2].t().transpose(1, 0).flatten().tolist())\n",
    "            y_train.append(out[i, agent_id,:, 0:2].t().transpose(1, 0).flatten().tolist())\n",
    "            \n",
    "        \n",
    "        predict_in = torch.FloatTensor(predict_in)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        \n",
    "        #print(predict_in.shape)\n",
    "        #print(y_train.shape)\n",
    "        \n",
    "        #clear out the gradients from the last step loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x_train = model(predict_in.type(torch.float))\n",
    "        \n",
    "        #RMSE as kaggle competition has it\n",
    "        loss = torch.sqrt(l(x_train, y_train.type(torch.float)))\n",
    "        if loss_ema < 0:\n",
    "            loss_ema = loss\n",
    "        loss_ema = loss_ema*0.99+loss*0.01\n",
    "            \n",
    "        #print(x_train)\n",
    "        #print(y_train)\n",
    "        #backward propagation: calculate gradients\n",
    "        loss.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if (epoch%25 == 0 or epoch == 499):\n",
    "            print('epoch {}, loss_ema {}, loss {}'.format(epoch, loss_ema.item(), loss.item()))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "agent_id = 0\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    for i_batch, sample_batch in enumerate(val_loader):\n",
    "        inp, out, track_id, agents = sample_batch\n",
    "        flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "        inp = inp #.cuda()\n",
    "        out = out #.cuda()\n",
    "        \n",
    "        batch_sz = inp.size(0)\n",
    "        agent_sz = inp.size(1)\n",
    "        \n",
    "        predict_in = []\n",
    "        y_train = []\n",
    "        \n",
    "        \n",
    "        for i in range(batch_sz):\n",
    "            \n",
    "            agent_id = numpy.where(flat_track_id[i][:] == (agents[i]))[0][0]\n",
    "            #if i ==0:\n",
    "                #print(agents[i])\n",
    "            predict_in.append(inp[i, agent_id,:, 0:2].t().float())\n",
    "            y_train.append(out[i, agent_id,:, 0:2].t().float())\n",
    "            \n",
    "        \n",
    "        predict_in = torch.stack(predict_in)\n",
    "        y_train = torch.stack(y_train)\n",
    "        \n",
    "        #print(predict_in.shape)\n",
    "        #print(y_train.shape)\n",
    "        \n",
    "        #clear out the gradients from the last step loss.backward()\n",
    "        \n",
    "        x_train = model.forward_test(predict_in)\n",
    "        \n",
    "        #RMSE as kaggle competition has it\n",
    "        loss = torch.sqrt(l(x_train, y_train))\n",
    "        \n",
    "        \n",
    "        test_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "        fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "        axs = axs.ravel()   \n",
    "        for i in range(batch_sz):\n",
    "            \n",
    "            axs[i].xaxis.set_ticks([])\n",
    "            axs[i].yaxis.set_ticks([])\n",
    "\n",
    "            # first two feature dimensions are (x,y) positions\n",
    "            axs[i].scatter(x_train[i,:,0], x_train[i,:,1])\n",
    "            axs[i].scatter(y_train[i,:,0], y_train[i,:,1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file_string = './output_batch_sz_128_linear.csv'\n",
    "\n",
    "labels = ['ID']\n",
    "for i in range(60):\n",
    "    labels.append('v' + str(i+1))\n",
    "\n",
    "\n",
    "write = True\n",
    "if (write):\n",
    "    with open(file_string, 'w', newline ='') as file:    \n",
    "        write = csv.writer(file)\n",
    "        write.writerow(labels)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "result = []\n",
    "#for epoch in range(13):\n",
    "for i_batch, sample_batch in enumerate(test_loader):\n",
    "    inp, track_id, agents = sample_batch\n",
    "    flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "    inp = inp #.cuda()\n",
    "        \n",
    "    batch_sz = inp.size(0)\n",
    "        \n",
    "    for i in range(batch_sz):\n",
    "        #print(i)    \n",
    "        agent_id = numpy.where(flat_track_id[i][:] == (agents[i]))[0][0]\n",
    "        predict_in = (inp[i, agent_id,:, 0:2].t().transpose(1, 0).flatten().tolist())\n",
    "        \n",
    "        predict_in = torch.FloatTensor(predict_in)\n",
    "        \n",
    "        x_train = model.forward_test(predict_in.type(torch.float))\n",
    "        \n",
    "        x_train = x_train.tolist()\n",
    "        \n",
    "        x_train.insert(0, float(agents[i][0].replace('-', '')))\n",
    "            \n",
    "        #print(x_train)\n",
    "        if (write):\n",
    "            with open(file_string, 'a+', newline ='') as file:    \n",
    "                write = csv.writer(file)\n",
    "                write.writerow(x_train)\n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, i 0, loss_ema 1655.3358154296875, loss 1655.3358154296875\n",
      "epoch 0, i 64, loss_ema 959.0897216796875, loss 73.25421905517578\n",
      "epoch 0, i 128, loss_ema 525.6539916992188, loss 23.224782943725586\n",
      "epoch 0, i 192, loss_ema 298.9344787597656, loss 52.1901741027832\n",
      "epoch 0, i 256, loss_ema 180.05484008789062, loss 30.195619583129883\n",
      "epoch 0, i 320, loss_ema 114.60310363769531, loss 62.52608108520508\n",
      "epoch 0, i 384, loss_ema 80.06090545654297, loss 34.43927764892578\n",
      "epoch 0, i 448, loss_ema 62.410709381103516, loss 38.41350555419922\n",
      "epoch 50, i 0, loss_ema 16.42490577697754, loss 22.071258544921875\n",
      "epoch 50, i 64, loss_ema 16.476022720336914, loss 7.4602227210998535\n",
      "epoch 50, i 128, loss_ema 17.123716354370117, loss 12.620437622070312\n",
      "epoch 50, i 192, loss_ema 18.516742706298828, loss 31.541954040527344\n",
      "epoch 50, i 256, loss_ema 17.12021827697754, loss 11.234814643859863\n",
      "epoch 50, i 320, loss_ema 17.299354553222656, loss 11.06428337097168\n",
      "epoch 50, i 384, loss_ema 15.93598747253418, loss 15.164043426513672\n",
      "epoch 50, i 448, loss_ema 17.165498733520508, loss 47.66084671020508\n",
      "epoch 59, i 511, loss_ema 15.239668846130371, loss 19.53322410583496\n",
      "epoch 0, i 0, loss 14.603276252746582\n",
      "epoch 0, i 64, loss 14.37785530090332\n",
      "epoch 0, i 128, loss 14.911050796508789\n",
      "epoch 0, i 192, loss 13.342683792114258\n",
      "epoch 0, i 256, loss 15.937043190002441\n",
      "epoch 0, i 320, loss 11.30461597442627\n",
      "epoch 0, i 384, loss 10.3917875289917\n",
      "epoch 0, i 448, loss 14.58070182800293\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = myModel() #.cuda()\n",
    "model.train()\n",
    "#learning_rate = 0.0000001\n",
    "learning_rate = 1e-4\n",
    "l = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr =learning_rate )\n",
    "loss_ema = -1\n",
    "\n",
    "losses = []\n",
    "losses_ema = []\n",
    "\n",
    "agent_id = 0\n",
    "for epoch in range(60):\n",
    "    for i_batch, sample_batch in enumerate(val_loader):\n",
    "        inp, out, track_id, agents = sample_batch\n",
    "        flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "        inp = inp #.cuda()\n",
    "        out = out #.cuda()\n",
    "        \n",
    "        batch_sz = inp.size(0)\n",
    "        agent_sz = inp.size(1)\n",
    "        \n",
    "        for i in range(batch_sz):\n",
    "            \n",
    "            agent_id = numpy.where(flat_track_id[i][:] == (agents[i]))[0][0]\n",
    "            #if i ==0:\n",
    "                #print(agents[i])\n",
    "            predict_in = inp[i, agent_id,:, 0:2].t()\n",
    "            \n",
    "            x_train = model(predict_in.type(torch.float))\n",
    "            \n",
    "            #calculate the loss\n",
    "            y_train = out[i, agent_id,:, 0:2].t()\n",
    "            \n",
    "            #RMSE as kaggle competition has it\n",
    "            loss = torch.sqrt(l(x_train, y_train.type(torch.float)))\n",
    "            if loss_ema < 0:\n",
    "                loss_ema = loss\n",
    "            loss_ema = loss_ema*0.99+loss*0.01\n",
    "            \n",
    "            #print(x_train)\n",
    "            #print(y_train)\n",
    "            #backward propagation: calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            #clear out the gradients from the last step loss.backward()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            losses_ema.append(loss_ema.item())\n",
    "        \n",
    "            if (epoch%50 == 0 and i%64 == 0):\n",
    "                print('epoch {}, i {}, loss_ema {}, loss {}'.format(epoch, i, loss_ema.item(), loss.item()))\n",
    "            \n",
    "        break   \n",
    "\n",
    "print('epoch {}, i {}, loss_ema {}, loss {}'.format(epoch, i, loss_ema.item(), loss.item()))\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    for i_batch, sample_batch in enumerate(val_loader):\n",
    "        inp, out, track_id, agents = sample_batch\n",
    "        flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "        inp = inp #.cuda()\n",
    "        out = out #.cuda()\n",
    "        \n",
    "        batch_sz = inp.size(0)\n",
    "        agent_sz = inp.size(1)\n",
    "        \n",
    "        for i in range(batch_sz):\n",
    "            \n",
    "            agent_id = numpy.where(flat_track_id[i][:] == (agents[i]))[0][0]\n",
    "            #if i ==0:\n",
    "                #print(agents[i])\n",
    "            predict_in = inp[i, agent_id,:, 0:2].t()\n",
    "            \n",
    "            x_train = model.forward_test(predict_in.type(torch.float))\n",
    "            \n",
    "            #calculate the loss\n",
    "            y_train = out[i, agent_id,:, 0:2].t()\n",
    "            \n",
    "            #RMSE as kaggle competition has it\n",
    "            loss = torch.sqrt(l(x_train, y_train.type(torch.float)))\n",
    "            if loss_ema < 0:\n",
    "                loss_ema = loss\n",
    "            loss_ema = loss_ema*0.99+loss*0.01\n",
    "            \n",
    "            #print(x_train)\n",
    "            #print(y_train)\n",
    "            #backward propagation: calculate gradients\n",
    "            #loss.backward()\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "            if (epoch%50 == 0 and i%64 == 0):\n",
    "                print('epoch {}, i {}, loss {}'.format(epoch, i, loss.item()))\n",
    "            \n",
    "        break\n",
    "\n",
    "\n",
    "plt.plot(losses, label=\"Training loss\")\n",
    "plt.plot(losses_ema, label=\"EMA  training loss\")\n",
    "plt.plot(test_losses, label=\"Validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19', 'v20', 'v21', 'v22', 'v23', 'v24', 'v25', 'v26', 'v27', 'v28', 'v29', 'v30', 'v31', 'v32', 'v33', 'v34', 'v35', 'v36', 'v37', 'v38', 'v39', 'v40', 'v41', 'v42', 'v43', 'v44', 'v45', 'v46', 'v47', 'v48', 'v49', 'v50', 'v51', 'v52', 'v53', 'v54', 'v55', 'v56', 'v57', 'v58', 'v59', 'v60']\n"
     ]
    }
   ],
   "source": [
    "labels = ['ID']\n",
    "for i in range(60):\n",
    "    labels.append('v' + str(i+1))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "file_string = './output_batch_sz_512_epoch_400.csv'\n",
    "\n",
    "write = True\n",
    "if (write):\n",
    "    with open(file_string, 'w', newline ='') as file:    \n",
    "        write = csv.writer(file)\n",
    "        write.writerow(labels)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "result = []\n",
    "#for epoch in range(13):\n",
    "for i_batch, sample_batch in enumerate(test_loader):\n",
    "    inp, track_id, agents = sample_batch\n",
    "    flat_track_id = numpy.array(track_id)[:][:, :, 0, 0]\n",
    "        \n",
    "    inp = inp #.cuda()\n",
    "        \n",
    "    batch_sz = inp.size(0)\n",
    "        \n",
    "    for i in range(batch_sz):\n",
    "            \n",
    "        agent_id = numpy.where(flat_track_id[i][:] == (agents[i]))[0][0]\n",
    "        \n",
    "        predict_in = inp[i, agent_id,:, 0:2].t()\n",
    "            \n",
    "        x_train = model.forward_test(predict_in.type(torch.float))\n",
    "        x_train = x_train.transpose(1, 0).flatten().tolist()\n",
    "            \n",
    "            #print(x_train)\n",
    "            #print(agents[i][0].replace('-', ''))\n",
    "        x_train.insert(0, float(agents[i][0].replace('-', '')))\n",
    "            \n",
    "            #print(x_train)\n",
    "        if (write):\n",
    "            with open(file_string, 'a+', newline ='') as file:    \n",
    "                write = csv.writer(file)\n",
    "                write.writerow(x_train)\n",
    "            \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "agent_id = 0\n",
    "\n",
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out, track_id = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])\n",
    "\n",
    "        \n",
    "        \n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out, track_id = sample_batch\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
